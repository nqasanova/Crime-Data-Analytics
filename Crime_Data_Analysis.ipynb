{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644b9867",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Uploading the data to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142ffec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: hdfs in /opt/anaconda3/lib/python3.11/site-packages (2.7.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: docopt in /opt/anaconda3/lib/python3.11/site-packages (from hdfs) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from hdfs) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9f313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch and upload process...\n",
      "\n",
      "Fetching data: Offset=0, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# HDFS client configuration\n",
    "hdfs_client = InsecureClient('http://localhost:9870', user='khaleed_mammad')\n",
    "hdfs_path = '/input/nypd_data/'\n",
    "\n",
    "# API endpoint\n",
    "api_url = \"https://data.cityofnewyork.us/resource/qgea-i56i.csv\"\n",
    "\n",
    "# Parameters for pagination\n",
    "limit = 50000  # Maximum rows per request\n",
    "offset = 0\n",
    "rows_to_fetch = 8900000 // 8  \n",
    "rows_fetched = 0\n",
    "\n",
    "\n",
    "max_retries = 5\n",
    "retry_delay = 5\n",
    "\n",
    "chunk_size = 100000  # Rows per chunk\n",
    "\n",
    "print(\"Starting data fetch and upload process...\\n\")\n",
    "\n",
    "# Fetch and upload the dataset in chunks\n",
    "while rows_fetched < rows_to_fetch:\n",
    "    params = {\"$limit\": limit, \"$offset\": offset}\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Fetching data: Offset={offset}, Limit={limit} (Attempt {retries + 1})...\")\n",
    "            response = requests.get(api_url, params=params, timeout=30)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "                retries += 1\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "\n",
    "            # Read the chunk into a DataFrame\n",
    "            chunk = pd.read_csv(StringIO(response.text))\n",
    "            if chunk.empty:\n",
    "                print(\"No more data to fetch. Exiting.\")\n",
    "                break\n",
    "\n",
    "            rows_fetched += len(chunk)\n",
    "            offset += limit\n",
    "\n",
    "            # Upload the chunk to HDFS\n",
    "            chunk_path = f'/input/nypd_data/NYPD_Complaint_Data_Chunk_{offset//limit}.csv'\n",
    "            with StringIO() as csv_buffer:\n",
    "                chunk.to_csv(csv_buffer, index=False)\n",
    "                hdfs_client.write(chunk_path, csv_buffer.getvalue(), overwrite=True)\n",
    "            print(f\"Uploaded chunk to HDFS: {chunk_path} (Total rows fetched: {rows_fetched})\\n\")\n",
    "            break\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Request timed out. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    if retries == max_retries:\n",
    "        print(f\"Max retries reached for Offset={offset}. Exiting.\")\n",
    "        break\n",
    "\n",
    "print(f\"Process complete. Total rows fetched: {rows_fetched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c98fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reading data and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf18a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import to_date \n",
    "from pyspark.sql.functions import year, month \n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8504079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 CSV files. Merging now...\n",
      "Reading file: NYPD_Complaint_Data_Chunk_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_11.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_13.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_14.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_4.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_7.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_1.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_19.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_20.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_21.csv\n",
      "All files merged successfully into: /Users/khaleed_mammad/Desktop/Khaleed/ADA_Semesters/Fall_2024/Big_Data/Project/NYPD_Complaint_Data_Combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "directory = '/Users/khaleed_mammad/Desktop/Khaleed/ADA_Semesters/Fall_2024/Big_Data/Project'\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(directory, 'NYPD_Complaint_Data_Combined.csv')\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Check if there are CSV files\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Merging now...\")\n",
    "\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through each CSV file and append its DataFrame to the list\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"Reading file: {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"All files merged successfully into: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222bd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYPD Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the local combined CSV file\n",
    "local_csv_path = output_file\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(local_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73884939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "|cmplnt_num|       cmplnt_fr_dt|       cmplnt_fr_tm|       cmplnt_to_dt|cmplnt_to_tm|addr_pct_cd|             rpt_dt|ky_cd|           ofns_desc|pd_cd|             pd_desc|crm_atpt_cptd_cd| law_cat_cd|      boro_nm|loc_of_occur_desc|       prem_typ_desc|         juris_desc|jurisdiction_code|parks_nm|hadevelopt|housing_psa|x_coord_cd|y_coord_cd|susp_age_group|     susp_race|susp_sex|transit_district| latitude| longitude|             lat_lon|         patrol_boro|station_name|vic_age_group|vic_race|vic_sex|\n",
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "| 265088253|2023-02-09 00:00:00|2024-11-22 15:00:00|2023-03-09 00:00:00|    15:00:00|       26.0|2023-03-14 00:00:00|  109|       GRAND LARCENY|439.0|LARCENY,GRAND FRO...|       COMPLETED|     FELONY|    MANHATTAN|         FRONT OF|      MAILBOX INSIDE|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  995804.0|  235548.0|       UNKNOWN|       UNKNOWN|       U|            NULL|40.813196|-73.958257|(40.813196, -73.9...|PATROL BORO MAN N...|      (null)|          65+|   WHITE|      M|\n",
      "| 265110558|2023-03-14 00:00:00|2024-11-22 21:12:00|2023-03-14 00:00:00|    21:12:00|       28.0|2023-03-14 00:00:00|  113|             FORGERY|729.0|FORGERY,ETC.,UNCL...|       COMPLETED|     FELONY|    MANHATTAN|         FRONT OF|              STREET|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  997571.0|  234556.0|         25-44|WHITE HISPANIC|       M|            NULL|40.810469|-73.951878|(40.810469, -73.9...|PATROL BORO MAN N...|      (null)|      UNKNOWN| UNKNOWN|      E|\n",
      "| 265096674|2023-03-14 00:00:00|2024-11-22 16:45:00|2023-03-14 00:00:00|    16:45:00|      122.0|2023-03-14 00:00:00|  118|   DANGEROUS WEAPONS|792.0|CRIMINAL POSSESSI...|       COMPLETED|     FELONY|STATEN ISLAND|         FRONT OF|RESIDENCE - PUBLI...|N.Y. HOUSING POLICE|                2|  (null)|    (null)|       1291|  962198.0|  155911.0|         25-44|         BLACK|       M|            NULL|40.594591|-74.079403|(40.594591, -74.0...|PATROL BORO STATE...|      (null)|      UNKNOWN| UNKNOWN|      E|\n",
      "| 265104725|2023-03-14 00:00:00|2024-11-22 17:30:00|2023-03-14 00:00:00|    17:57:00|        9.0|2023-03-14 00:00:00|  361|OFF. AGNST PUB OR...|661.0|     LEWDNESS,PUBLIC|       COMPLETED|MISDEMEANOR|    MANHATTAN|           INSIDE|    RESTAURANT/DINER|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  986428.0|  205224.0|         25-44|         BLACK|       M|            NULL|40.729971|-73.992138|(40.729971, -73.9...|PATROL BORO MAN S...|      (null)|      UNKNOWN| UNKNOWN|      F|\n",
      "| 265096568|2023-03-14 00:00:00|2024-11-22 12:00:00|2023-03-14 00:00:00|    12:10:00|        6.0|2023-03-14 00:00:00|  112|         THEFT-FRAUD|739.0|FRAUD,UNCLASSIFIE...|       COMPLETED|     FELONY|    MANHATTAN|           INSIDE|RESIDENCE - APT. ...|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  983857.0|  205980.0|       UNKNOWN|       UNKNOWN|       U|            NULL|40.732047|-74.001415|(40.732047, -74.0...|PATROL BORO MAN S...|      (null)|        45-64|   WHITE|      M|\n",
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de404f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cmplnt_num='265088253', cmplnt_fr_dt=datetime.datetime(2023, 2, 9, 0, 0), cmplnt_fr_tm=datetime.datetime(2024, 11, 22, 15, 0), cmplnt_to_dt=datetime.datetime(2023, 3, 9, 0, 0), cmplnt_to_tm='15:00:00', addr_pct_cd=26.0, rpt_dt=datetime.datetime(2023, 3, 14, 0, 0), ky_cd=109, ofns_desc='GRAND LARCENY', pd_cd=439.0, pd_desc='LARCENY,GRAND FROM OPEN AREAS, UNATTENDED', crm_atpt_cptd_cd='COMPLETED', law_cat_cd='FELONY', boro_nm='MANHATTAN', loc_of_occur_desc='FRONT OF', prem_typ_desc='MAILBOX INSIDE', juris_desc='N.Y. POLICE DEPT', jurisdiction_code=0, parks_nm='(null)', hadevelopt='(null)', housing_psa='(null)', x_coord_cd=995804.0, y_coord_cd=235548.0, susp_age_group='UNKNOWN', susp_race='UNKNOWN', susp_sex='U', transit_district=None, latitude=40.813196, longitude=-73.958257, lat_lon='(40.813196, -73.958257)', patrol_boro='PATROL BORO MAN NORTH', station_name='(null)', vic_age_group='65+', vic_race='WHITE', vic_sex='M')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e3557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cmplnt_num: string (nullable = true)\n",
      " |-- cmplnt_fr_dt: timestamp (nullable = true)\n",
      " |-- cmplnt_fr_tm: timestamp (nullable = true)\n",
      " |-- cmplnt_to_dt: timestamp (nullable = true)\n",
      " |-- cmplnt_to_tm: string (nullable = true)\n",
      " |-- addr_pct_cd: double (nullable = true)\n",
      " |-- rpt_dt: timestamp (nullable = true)\n",
      " |-- ky_cd: integer (nullable = true)\n",
      " |-- ofns_desc: string (nullable = true)\n",
      " |-- pd_cd: double (nullable = true)\n",
      " |-- pd_desc: string (nullable = true)\n",
      " |-- crm_atpt_cptd_cd: string (nullable = true)\n",
      " |-- law_cat_cd: string (nullable = true)\n",
      " |-- boro_nm: string (nullable = true)\n",
      " |-- loc_of_occur_desc: string (nullable = true)\n",
      " |-- prem_typ_desc: string (nullable = true)\n",
      " |-- juris_desc: string (nullable = true)\n",
      " |-- jurisdiction_code: integer (nullable = true)\n",
      " |-- parks_nm: string (nullable = true)\n",
      " |-- hadevelopt: string (nullable = true)\n",
      " |-- housing_psa: string (nullable = true)\n",
      " |-- x_coord_cd: double (nullable = true)\n",
      " |-- y_coord_cd: double (nullable = true)\n",
      " |-- susp_age_group: string (nullable = true)\n",
      " |-- susp_race: string (nullable = true)\n",
      " |-- susp_sex: string (nullable = true)\n",
      " |-- transit_district: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lat_lon: string (nullable = true)\n",
      " |-- patrol_boro: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- vic_age_group: string (nullable = true)\n",
      " |-- vic_race: string (nullable = true)\n",
      " |-- vic_sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0644c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, cmplnt_num: string, cmplnt_to_tm: string, addr_pct_cd: string, ky_cd: string, ofns_desc: string, pd_cd: string, pd_desc: string, crm_atpt_cptd_cd: string, law_cat_cd: string, boro_nm: string, loc_of_occur_desc: string, prem_typ_desc: string, juris_desc: string, jurisdiction_code: string, parks_nm: string, hadevelopt: string, housing_psa: string, x_coord_cd: string, y_coord_cd: string, susp_age_group: string, susp_race: string, susp_sex: string, transit_district: string, latitude: string, longitude: string, lat_lon: string, patrol_boro: string, station_name: string, vic_age_group: string, vic_race: string, vic_sex: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d626b801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmplnt_num',\n",
       " 'cmplnt_fr_dt',\n",
       " 'cmplnt_fr_tm',\n",
       " 'cmplnt_to_dt',\n",
       " 'cmplnt_to_tm',\n",
       " 'addr_pct_cd',\n",
       " 'rpt_dt',\n",
       " 'ky_cd',\n",
       " 'ofns_desc',\n",
       " 'pd_cd',\n",
       " 'pd_desc',\n",
       " 'crm_atpt_cptd_cd',\n",
       " 'law_cat_cd',\n",
       " 'boro_nm',\n",
       " 'loc_of_occur_desc',\n",
       " 'prem_typ_desc',\n",
       " 'juris_desc',\n",
       " 'jurisdiction_code',\n",
       " 'parks_nm',\n",
       " 'hadevelopt',\n",
       " 'housing_psa',\n",
       " 'x_coord_cd',\n",
       " 'y_coord_cd',\n",
       " 'susp_age_group',\n",
       " 'susp_race',\n",
       " 'susp_sex',\n",
       " 'transit_district',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'lat_lon',\n",
       " 'patrol_boro',\n",
       " 'station_name',\n",
       " 'vic_age_group',\n",
       " 'vic_race',\n",
       " 'vic_sex']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea75e83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f869e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       cmplnt_fr_dt|\n",
      "+-------------------+\n",
      "|2023-02-09 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cmplnt_fr_dt\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bf47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we create columns to store 'Year, month, hour and day of the week' of start date of complaint\n",
    "df = df.withColumn('Start_Date', to_timestamp('cmplnt_fr_dt', 'MM/dd/yyyy'))\n",
    "df = df.withColumn('Date', to_date('Start_Date'))\n",
    "df = df.withColumn('Hour', hour(df['cmplnt_fr_tm']))\n",
    "df = df.withColumn('YEAR', year('Date')) \n",
    "df = df.withColumn('MONTH', month('Date'))\n",
    "df = df.withColumn(\"DAY_of_WEEK\", date_format(\"Date\", \"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d89f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|vic_age_group| count|\n",
      "+-------------+------+\n",
      "|          <18| 43353|\n",
      "|        25-44|409647|\n",
      "|      UNKNOWN|326170|\n",
      "|         1018|     2|\n",
      "|          -49|     1|\n",
      "|          949|     1|\n",
      "|          65+| 58291|\n",
      "|         -968|     1|\n",
      "|        18-24| 95680|\n",
      "|          -33|     1|\n",
      "|           -3|     2|\n",
      "|        45-64|216793|\n",
      "|           -2|     3|\n",
      "|          -30|     3|\n",
      "|           -6|     2|\n",
      "|          -57|     1|\n",
      "|          950|     1|\n",
      "|         -945|     1|\n",
      "|         -963|     1|\n",
      "|          -27|     1|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb86946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can categorize age groups --> Teenager, Young Adult, Middle Age, Mid Old\n",
    "df1 = df1.withColumn('VIC_AGE_Cat',\n",
    "    when(df1.VIC_AGE_GROUP == '<18', 'Teenager')\n",
    "    .when(df1.VIC_AGE_GROUP == '18-25', 'Young Adult')\n",
    "    .when(df1.VIC_AGE_GROUP == '25-44', 'Middle Age')\n",
    "    .when(df1.VIC_AGE_GROUP == '44-64', 'Mid Old')\n",
    "    .otherwise('Senior'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2815adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# We can categorize age groups --> Teenager, Young Adult, Middle Age, Mid Old and Senior\n",
    "\n",
    "# As UNKNOWN occurs 326170 times, we replace it with mode instead of instead of deleting (but we delete invalid ages)\n",
    "\n",
    "mode_value = (\n",
    "    df.filter(df[\"vic_age_group\"] != \"UNKNOWN\")\n",
    "    .groupBy(\"vic_age_group\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .first()[\"vic_age_group\"]  # Get the value of the most frequent category\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"vic_age_group\",\n",
    "    when(df[\"vic_age_group\"] == \"UNKNOWN\", mode_value)  # Replace UNKNOWN with mode\n",
    "    .when(df[\"vic_age_group\"].isin(\"<18\", \"18-24\", \"25-44\", \"45-64\", \"65+\"), df[\"vic_age_group\"])  # Keep valid categories\n",
    "    .otherwise(None)  # Mark invalid values as NULL for removal\n",
    ")\n",
    "\n",
    "df = df.filter(df[\"vic_age_group\"].isNotNull())\n",
    "\n",
    "# Now, we can categorize\n",
    "df = df.withColumn(\n",
    "    \"vic_age_category\",\n",
    "    when(df[\"vic_age_group\"] == \"<18\", \"Teenager\")\n",
    "    .when(df[\"vic_age_group\"] == \"18-24\", \"Young Adult\")\n",
    "    .when(df[\"vic_age_group\"] == \"25-44\", \"Middle Age\")\n",
    "    .when(df[\"vic_age_group\"] == \"45-64\", \"Mid Old\")\n",
    "    .when(df[\"vic_age_group\"] == \"65+\", \"Senior\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "811808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|vic_age_group| count|\n",
      "+-------------+------+\n",
      "|          <18| 43353|\n",
      "|        25-44|735817|\n",
      "|          65+| 58291|\n",
      "|        18-24| 95680|\n",
      "|        45-64|216793|\n",
      "+-------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50877071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|vic_age_category| count|\n",
      "+----------------+------+\n",
      "|          Senior| 58291|\n",
      "|        Teenager| 43353|\n",
      "|      Middle Age|735817|\n",
      "|     Young Adult| 95680|\n",
      "|         Mid Old|216793|\n",
      "+----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c176f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to drop some irrelevant and redundant columns (some are too specific and some of them have too many null values)\n",
    "dataset = df.drop('cmplnt_num','cmplnt_fr_dt', 'cmplnt_fr_tm', 'cmplnt_to_dt', 'cmplnt_to_tm','pd_cd','rpt_dt',\n",
    "                 'pd_desc','loc_of_occur_desc', 'prem_typ_desc', 'juris_desc', 'jurisdiction_code', 'parks_nm',\n",
    "                  'hadevelopt', 'housing_psa','x_coord_cd','y_coord_cd','susp_age_group','susp_race', \n",
    "                  'susp_sex', 'transit_district','patrol_boro','station_name','vic_age_group','vic_race', 'Start_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f8f0cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "|addr_pct_cd|ky_cd|    ofns_desc|crm_atpt_cptd_cd|law_cat_cd|  boro_nm| latitude| longitude|             lat_lon|vic_sex|      Date|Hour|YEAR|MONTH|DAY_of_WEEK|vic_age_category|\n",
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "|       26.0|  109|GRAND LARCENY|       COMPLETED|    FELONY|MANHATTAN|40.813196|-73.958257|(40.813196, -73.9...|      M|2023-02-09|  15|2023|    2|        Thu|          Senior|\n",
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "710a0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the columns for better readabiliy\n",
    "\n",
    "dataframe = dataset.toDF('Neighborhood', 'Offence_Code', 'Offence_Type',\n",
    "        'Status','Offence_Level','Borough','Latitude','Longitude','Lat_Lon', 'Victim_Sex', 'Date', 'Hour', 'Year', 'Month', 'Day_of_Week','Victim_Agegroup')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48ca4973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We also handle with null values\n",
    "\n",
    "mean_long = dataframe.agg({'Longitude': 'mean'}).collect()[0][0] \n",
    "dataframe = dataframe.fillna(mean_long, subset=['Longitude'])\n",
    "\n",
    "mean_lat = dataframe.agg({'Latitude': 'mean'}).collect()[0][0] \n",
    "dataframe = dataframe.fillna(mean_lat, subset=['Latitude'])\n",
    "\n",
    "dataframe = dataframe.fillna(-1, subset=['Borough','Neighborhood','Offence_Code','Offence_Type','Status','Offence_level','Victim_Sex'])\n",
    "dataframe = dataframe.na.drop('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d958e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous dataset: 1149934 rows \n",
      "New dataset: 1149919 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:=====================>                                    (3 + 5) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Previous dataset: {df.count()} rows \\nNew dataset: {dataframe.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a6d54167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We save the dataset\n",
    "dataframe.write.csv(\"cleaned_nypd_data.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb769317",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analysis with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32707091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 13:12:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYPD Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f8f2492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 110:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Neighborhood: double (nullable = true)\n",
      " |-- Offence_Code: integer (nullable = true)\n",
      " |-- Offence_Type: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Offence_Level: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Lat_Lon: string (nullable = true)\n",
      " |-- Victim_Sex: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day_of_Week: string (nullable = true)\n",
      " |-- Victim_Agegroup: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 110:=======>                                                 (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_data = spark.read.csv(\"cleaned_nypd_data.csv\", header=True, inferSchema=True)\n",
    "cleaned_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d036103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database nypddatabase\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5db5d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n",
    "spark.sql(\"USE nypddatabase\")\n",
    "cleaned_data.write.mode(\"overwrite\").saveAsTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fec8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "cleaned_data.createOrReplaceTempView(\"nypd_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d40b3d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|   namespace|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|nypddatabase|       df|      false|\n",
      "|            |nypd_data|       true|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42967cbd",
   "metadata": {},
   "source": [
    "### Overview Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4009f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|      1149919|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_records = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS total_records FROM df\n",
    "\"\"\")\n",
    "total_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99efce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|      Borough| count|\n",
      "+-------------+------+\n",
      "|     BROOKLYN|319820|\n",
      "|    MANHATTAN|280934|\n",
      "|       QUEENS|251879|\n",
      "|        BRONX|244984|\n",
      "|STATEN ISLAND| 50043|\n",
      "|       (null)|  2259|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "borough_count = spark.sql(\"\"\"\n",
    "    SELECT Borough, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Borough\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "borough_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c75b22a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We have null values, we can remove them\n",
    "cleaned_data = cleaned_data.filter(cleaned_data[\"Borough\"].isin('BROOKLYN', 'MANHATTAN', 'QUEENS', 'BRONX', 'STATEN ISLAND'))\n",
    "cleaned_data.write.mode(\"overwrite\").saveAsTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd25196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|      Borough| count|\n",
      "+-------------+------+\n",
      "|     BROOKLYN|319820|\n",
      "|    MANHATTAN|280934|\n",
      "|       QUEENS|251879|\n",
      "|        BRONX|244984|\n",
      "|STATEN ISLAND| 50043|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "borough_count = spark.sql(\"\"\"\n",
    "    SELECT Borough, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Borough\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "borough_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd8d410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        Offence_Type| count|\n",
      "+--------------------+------+\n",
      "|       PETIT LARCENY|236696|\n",
      "|       HARRASSMENT 2|176681|\n",
      "|ASSAULT 3 & RELAT...|119670|\n",
      "|       GRAND LARCENY|110396|\n",
      "|CRIMINAL MISCHIEF...| 92419|\n",
      "|      FELONY ASSAULT| 56915|\n",
      "|OFF. AGNST PUB OR...| 38685|\n",
      "|VEHICLE AND TRAFF...| 37389|\n",
      "|             ROBBERY| 36437|\n",
      "|MISCELLANEOUS PEN...| 35139|\n",
      "|            BURGLARY| 31688|\n",
      "|GRAND LARCENY OF ...| 31077|\n",
      "|     DANGEROUS DRUGS| 22483|\n",
      "|          SEX CRIMES| 18084|\n",
      "|   DANGEROUS WEAPONS| 15627|\n",
      "|OFFENSES AGAINST ...| 14447|\n",
      "|             FORGERY| 11989|\n",
      "|         THEFT-FRAUD|  7964|\n",
      "|OFFENSES INVOLVIN...|  6807|\n",
      "|INTOXICATED & IMP...|  6592|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "offence_type_distribution = spark.sql(\"\"\"\n",
    "    SELECT Offence_Type, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Offence_Type\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "offence_type_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fff1c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|Offence_Level| Count|\n",
      "+-------------+------+\n",
      "|       FELONY|388236|\n",
      "|  MISDEMEANOR|579643|\n",
      "|    VIOLATION|179781|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd9b8039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|   Status|  count|\n",
      "+---------+-------+\n",
      "|COMPLETED|1130934|\n",
      "|ATTEMPTED|  16726|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "status_distribution = spark.sql(\"\"\"\n",
    "    SELECT Status, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Status\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "status_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a3ba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e30d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|Offence_Level|Victim_Sex| Count|\n",
      "+-------------+----------+------+\n",
      "|  MISDEMEANOR|         D|138374|\n",
      "|  MISDEMEANOR|         M|180082|\n",
      "|       FELONY|         M|154396|\n",
      "|       FELONY|         L|  4248|\n",
      "|       FELONY|         D| 55341|\n",
      "|    VIOLATION|         D|  1650|\n",
      "|       FELONY|         F|133412|\n",
      "|  MISDEMEANOR|         L|  1613|\n",
      "|    VIOLATION|         F|112422|\n",
      "|       FELONY|         E| 40838|\n",
      "|    VIOLATION|         L|   197|\n",
      "|  MISDEMEANOR|         F|194506|\n",
      "|  MISDEMEANOR|         E| 65068|\n",
      "|    VIOLATION|         E|  2059|\n",
      "|    VIOLATION|         M| 63453|\n",
      "|       FELONY|         U|     1|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level,Victim_Sex,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level,Victim_Sex\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79185fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like we have some errors in sex column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67379359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Victim_Sex| Count|\n",
      "+----------+------+\n",
      "|         U|     1|\n",
      "|         L|  6058|\n",
      "|         E|107965|\n",
      "|         D|195365|\n",
      "|         M|397931|\n",
      "|         F|440340|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Victim_Sex,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Victim_Sex\n",
    "    ORDER BY Count\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18519fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping U, L, E , D would cause a problem as they contain big protion of our data. Therefore, \n",
    "# we decide to keep it since our purpose is not studying crime patterns by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ad47974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Count|\n",
      "+----+-----+\n",
      "|1011|    3|\n",
      "|1012|    6|\n",
      "|1013|    2|\n",
      "|1018|    1|\n",
      "|1021|    2|\n",
      "|1022|   11|\n",
      "|1023|   23|\n",
      "|1900|    1|\n",
      "|1921|    2|\n",
      "|1922|    7|\n",
      "|1923|    5|\n",
      "|1949|    1|\n",
      "|1955|    1|\n",
      "|1961|    1|\n",
      "|1964|    1|\n",
      "|1966|    1|\n",
      "|1967|    1|\n",
      "|1969|    1|\n",
      "|1970|    1|\n",
      "|1971|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Year, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Year\n",
    "    ORDER BY Year\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d262a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like we have errors in year column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc1c36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Year| Count|\n",
      "+----+------+\n",
      "|2023|542304|\n",
      "|2022|528717|\n",
      "|2021| 71748|\n",
      "|2020|  1546|\n",
      "|2019|   686|\n",
      "|2018|   438|\n",
      "|2017|   304|\n",
      "|2016|   243|\n",
      "|2015|   215|\n",
      "|2014|   186|\n",
      "|2013|   189|\n",
      "|2012|   140|\n",
      "|2011|   132|\n",
      "|2010|    68|\n",
      "|2009|    47|\n",
      "|2008|    41|\n",
      "|2007|    52|\n",
      "|2006|    41|\n",
      "|2005|    28|\n",
      "|2004|    28|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Year, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Year\n",
    "    ORDER BY Year DESC\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e13bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Since most of data smaples belong to the years \"2023\", \"2022\", \"2021\" and \"2020\", we only keep those.\n",
    "cleaned_data = cleaned_data.filter(cleaned_data[\"Year\"].isin(2023, 2022, 2021, 2020))\n",
    "cleaned_data.write.mode(\"overwrite\").saveAsTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "01c032a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Month| count|\n",
      "+-----+------+\n",
      "|   12|124920|\n",
      "|   11|112746|\n",
      "|    7| 96582|\n",
      "|   10| 96200|\n",
      "|    8| 95424|\n",
      "|    6| 95044|\n",
      "|    5| 94922|\n",
      "|    9| 91334|\n",
      "|    3| 88998|\n",
      "|    4| 87050|\n",
      "|    1| 83674|\n",
      "|    2| 77421|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "offences_per_month = spark.sql(\"\"\"\n",
    "    SELECT Month, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Month\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "offences_per_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe1aaf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|Day_of_Week| count|\n",
      "+-----------+------+\n",
      "|        Fri|172032|\n",
      "|        Wed|170882|\n",
      "|        Thu|166517|\n",
      "|        Tue|165789|\n",
      "|        Sat|159516|\n",
      "|        Mon|158996|\n",
      "|        Sun|150583|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "offences_per_day = spark.sql(\"\"\"\n",
    "    SELECT Day_of_Week, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Day_of_Week\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "offences_per_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdc3de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Hour|count|\n",
      "+----+-----+\n",
      "|  17|69816|\n",
      "|  15|69060|\n",
      "|  18|68168|\n",
      "|  16|67833|\n",
      "|  12|66067|\n",
      "|  19|64659|\n",
      "|  14|61797|\n",
      "|  20|61592|\n",
      "|  13|56167|\n",
      "|  21|54421|\n",
      "|  11|50699|\n",
      "|  22|49488|\n",
      "|   0|49414|\n",
      "|  10|49166|\n",
      "|   9|45434|\n",
      "|  23|43103|\n",
      "|   8|43100|\n",
      "|   1|34173|\n",
      "|   2|28889|\n",
      "|   7|28332|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "offences_by_hour = spark.sql(\"\"\"\n",
    "    SELECT Hour, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Hour\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "offences_by_hour.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2ca34",
   "metadata": {},
   "source": [
    "### Demographics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4269015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Victim_Agegroup| count|\n",
      "+---------------+------+\n",
      "|     Middle Age|732887|\n",
      "|        Mid Old|216110|\n",
      "|    Young Adult| 94895|\n",
      "|         Senior| 58087|\n",
      "|       Teenager| 42336|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "victim_agegroup_distribution = spark.sql(\"\"\"\n",
    "    SELECT Victim_Agegroup, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Victim_Agegroup\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "victim_agegroup_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28ae4310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Victim_Sex| count|\n",
      "+----------+------+\n",
      "|         F|438158|\n",
      "|         M|397152|\n",
      "|         D|195194|\n",
      "|         E|107753|\n",
      "|         L|  6058|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "victim_sex_distribution = spark.sql(\"\"\"\n",
    "    SELECT Victim_Sex, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Victim_Sex\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "victim_sex_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "21c86845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+-----+\n",
      "|Victim_Agegroup|Victim_Sex|        Offence_Type|count|\n",
      "+---------------+----------+--------------------+-----+\n",
      "|        Mid Old|         D|       PETIT LARCENY|   94|\n",
      "|        Mid Old|         D|       GRAND LARCENY|   14|\n",
      "|        Mid Old|         D|CRIMINAL MISCHIEF...|   12|\n",
      "|        Mid Old|         D|            BURGLARY|   10|\n",
      "|        Mid Old|         D|             ROBBERY|    3|\n",
      "|        Mid Old|         F|       HARRASSMENT 2|29928|\n",
      "|        Mid Old|         F|       PETIT LARCENY|14811|\n",
      "|        Mid Old|         F|ASSAULT 3 & RELAT...|11623|\n",
      "|        Mid Old|         F|       GRAND LARCENY|11534|\n",
      "|        Mid Old|         F|CRIMINAL MISCHIEF...| 8992|\n",
      "|        Mid Old|         F|OFF. AGNST PUB OR...| 4605|\n",
      "|        Mid Old|         F|MISCELLANEOUS PEN...| 4367|\n",
      "|        Mid Old|         F|      FELONY ASSAULT| 3666|\n",
      "|        Mid Old|         F|GRAND LARCENY OF ...| 2539|\n",
      "|        Mid Old|         F|            BURGLARY| 1877|\n",
      "|        Mid Old|         F|             ROBBERY| 1665|\n",
      "|        Mid Old|         F|VEHICLE AND TRAFF...| 1613|\n",
      "|        Mid Old|         F|OFFENSES AGAINST ...| 1450|\n",
      "|        Mid Old|         F|         THEFT-FRAUD| 1369|\n",
      "|        Mid Old|         F|          SEX CRIMES|  869|\n",
      "+---------------+----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "demographics_offence_correlation = spark.sql(\"\"\"\n",
    "    SELECT Victim_Agegroup, Victim_Sex, Offence_Type, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Victim_Agegroup, Victim_Sex, Offence_Type\n",
    "    ORDER BY Victim_Agegroup, Victim_Sex, count DESC\n",
    "\"\"\")\n",
    "demographics_offence_correlation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24a925ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+------+\n",
      "|Offence_Level|Year| Count|\n",
      "+-------------+----+------+\n",
      "|    VIOLATION|2021| 10648|\n",
      "|       FELONY|2022|177119|\n",
      "|  MISDEMEANOR|2021| 33790|\n",
      "|       FELONY|2021| 27310|\n",
      "|  MISDEMEANOR|2022|267665|\n",
      "|    VIOLATION|2020|   128|\n",
      "|  MISDEMEANOR|2023|276393|\n",
      "|       FELONY|2020|   831|\n",
      "|    VIOLATION|2023| 84875|\n",
      "|       FELONY|2023|181036|\n",
      "|  MISDEMEANOR|2020|   587|\n",
      "|    VIOLATION|2022| 83933|\n",
      "+-------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level,Year,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level,Year\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f91e7b",
   "metadata": {},
   "source": [
    "### Fairness Assessment Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "29b7a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+\n",
      "|Victim_Sex|Offence_Level| count|\n",
      "+----------+-------------+------+\n",
      "|         D|    VIOLATION|  1648|\n",
      "|         D|  MISDEMEANOR|138308|\n",
      "|         D|       FELONY| 55238|\n",
      "|         E|    VIOLATION|  2059|\n",
      "|         E|  MISDEMEANOR| 65051|\n",
      "|         E|       FELONY| 40643|\n",
      "|         F|    VIOLATION|112287|\n",
      "|         F|  MISDEMEANOR|193664|\n",
      "|         F|       FELONY|132207|\n",
      "|         L|    VIOLATION|   197|\n",
      "|         L|  MISDEMEANOR|  1613|\n",
      "|         L|       FELONY|  4248|\n",
      "|         M|    VIOLATION| 63393|\n",
      "|         M|  MISDEMEANOR|179799|\n",
      "|         M|       FELONY|153960|\n",
      "+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "sex_offencelevel_distribution = spark.sql(\"\"\"\n",
    "    SELECT Victim_Sex, Offence_Level, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Victim_Sex, Offence_Level\n",
    "    ORDER BY Victim_Sex, Offence_Level DESC\n",
    "\"\"\")\n",
    "sex_offencelevel_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3535c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------+\n",
      "|  Borough|Victim_Agegroup| count|\n",
      "+---------+---------------+------+\n",
      "|    BRONX|    Young Adult| 21338|\n",
      "|    BRONX|       Teenager| 10749|\n",
      "|    BRONX|         Senior| 10653|\n",
      "|    BRONX|     Middle Age|153115|\n",
      "|    BRONX|        Mid Old| 48392|\n",
      "| BROOKLYN|    Young Adult| 26857|\n",
      "| BROOKLYN|       Teenager| 12261|\n",
      "| BROOKLYN|         Senior| 16274|\n",
      "| BROOKLYN|     Middle Age|201673|\n",
      "| BROOKLYN|        Mid Old| 61795|\n",
      "|MANHATTAN|    Young Adult| 22821|\n",
      "|MANHATTAN|       Teenager|  6570|\n",
      "|MANHATTAN|         Senior| 13225|\n",
      "|MANHATTAN|     Middle Age|194585|\n",
      "|MANHATTAN|        Mid Old| 43024|\n",
      "|   QUEENS|    Young Adult| 19970|\n",
      "|   QUEENS|       Teenager| 10355|\n",
      "|   QUEENS|         Senior| 14642|\n",
      "|   QUEENS|     Middle Age|153917|\n",
      "|   QUEENS|        Mid Old| 52193|\n",
      "+---------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "agegroup_borough_distribution = spark.sql(\"\"\"\n",
    "    SELECT Borough, Victim_Agegroup, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Borough, Victim_Agegroup\n",
    "    ORDER BY Borough, Victim_Agegroup DESC\n",
    "\"\"\")\n",
    "agegroup_borough_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fde33e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------+------+\n",
      "|Victim_Sex|Victim_Agegroup|   Status| count|\n",
      "+----------+---------------+---------+------+\n",
      "|         D|        Mid Old|COMPLETED|   133|\n",
      "|         D|     Middle Age|COMPLETED|191654|\n",
      "|         D|     Middle Age|ATTEMPTED|  2360|\n",
      "|         D|         Senior|COMPLETED|    15|\n",
      "|         D|         Senior|ATTEMPTED|     1|\n",
      "|         D|    Young Adult|COMPLETED|  1031|\n",
      "|         E|     Middle Age|COMPLETED|106786|\n",
      "|         E|     Middle Age|ATTEMPTED|   964|\n",
      "|         E|         Senior|COMPLETED|     1|\n",
      "|         E|       Teenager|COMPLETED|     1|\n",
      "|         E|    Young Adult|COMPLETED|     1|\n",
      "|         F|        Mid Old|COMPLETED|102254|\n",
      "|         F|        Mid Old|ATTEMPTED|  1350|\n",
      "|         F|     Middle Age|COMPLETED|225105|\n",
      "|         F|     Middle Age|ATTEMPTED|  2620|\n",
      "|         F|         Senior|COMPLETED| 28609|\n",
      "|         F|         Senior|ATTEMPTED|   461|\n",
      "|         F|       Teenager|COMPLETED| 23283|\n",
      "|         F|       Teenager|ATTEMPTED|   311|\n",
      "|         F|    Young Adult|COMPLETED| 53509|\n",
      "+----------+---------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "demographics_status_correlation = spark.sql(\"\"\"\n",
    "    SELECT Victim_Sex, Victim_Agegroup, Status, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY Victim_Sex, Victim_Agegroup, Status\n",
    "    ORDER BY Victim_Sex, Victim_Agegroup, Status DESC\n",
    "\"\"\")\n",
    "demographics_status_correlation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6769c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|      1144315|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_records = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS total_records FROM df\n",
    "\"\"\")\n",
    "total_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4bd29cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_data.write.csv(\"ready_data.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f7908",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea61e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYPD Offence Level Prediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfd755e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_data = spark.read.csv(\"Git_version/ready_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ecbc942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146573"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "606985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80ca560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Neighborhood (double): 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Code (int): 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Type (string): 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 387:==========================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Status (string): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 393:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Level (string): 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Borough (string): 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Latitude (double): 96433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Longitude (double): 97259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Lat_Lon (string): 102112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Victim_Sex (string): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Date (date): 1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Hour (int): 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Year (int): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Month (int): 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Day_of_Week (string): 7\n",
      "unique number of Victim_Agegroup (string): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 459:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "str_featues = []\n",
    "num_features = []\n",
    "\n",
    "for feature in final_data.dtypes:\n",
    "    \n",
    "    # print unique value and data type for each feature\n",
    "    feature_name = str(feature[0])\n",
    "    feature_type = str(feature[1])\n",
    "    uni = final_data.select(feature_name).distinct().count()\n",
    "    print(\"unique number of {} ({}):\".format(feature_name, feature_type),uni)\n",
    "    \n",
    "    # find out features with integer type\n",
    "    if feature_type == \"int\" or feature_type == \"decimal(65,0)\":\n",
    "        \n",
    "        num_features.append(feature_name)\n",
    "        \n",
    "    # find out string feature and it's unique value less than 70\n",
    "    elif feature_type == \"string\" and uni < 70:\n",
    "        \n",
    "        str_featues.append(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "861fda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4 numerical features:['Offence_Code', 'Hour', 'Year', 'Month']\n",
      "Total 7 string features:['Offence_Type', 'Status', 'Offence_Level', 'Borough', 'Victim_Sex', 'Day_of_Week', 'Victim_Agegroup']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total {} numerical features:{}\".format(len(num_features),num_features)) \n",
    "print(\"Total {} string features:{}\".format(len(str_featues),str_featues)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e227e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Offence_Code: 0\n",
      "Missing values in Hour: 0\n",
      "Missing values in Year: 0\n",
      "Missing values in Month: 0\n",
      "Missing values in Offence_Type: 0\n",
      "Missing values in Status: 0\n",
      "Missing values in Offence_Level: 0\n",
      "Missing values in Borough: 0\n",
      "Missing values in Victim_Sex: 0\n",
      "Missing values in Day_of_Week: 0\n",
      "Missing values in Victim_Agegroup: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for missing values in numerical features\n",
    "for col_name in num_features:\n",
    "    missing_count = final_data.filter(col(col_name).isNull()).count()\n",
    "    print(f\"Missing values in {col_name}: {missing_count}\")\n",
    "\n",
    "# Check for missing values in categorical features\n",
    "for col_name in categorical_columns:\n",
    "    missing_count = final_data.filter(col(col_name).isNull()).count()\n",
    "    print(f\"Missing values in {col_name}: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43029d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 503:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 1146573\n",
      "Distinct Records: 1128427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 503:===================>                                     (3 + 6) / 9]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count total records\n",
    "total_records = final_data.count()\n",
    "\n",
    "# Count distinct records\n",
    "distinct_records = final_data.dropDuplicates().count()\n",
    "\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Distinct Records: {distinct_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52ee298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "044f2f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 513:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 1128427\n",
      "Distinct Records: 1128427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 515:======>                                                  (1 + 8) / 9]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count total records\n",
    "total_records = final_data.count()\n",
    "\n",
    "# Count distinct records\n",
    "distinct_records = final_data.dropDuplicates().count()\n",
    "\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Distinct Records: {distinct_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a946b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert categorical string features into numerical indices:\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in str_featues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "104b9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to prepare a feature vector for machine learning in PySpark\n",
    "inputs = [\"{}_index\".format(x) for x in str_featues] + num_features\n",
    "feature_vector = VectorAssembler(inputCols= list(set(inputs) - set(['Offence_Level_index'])), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b652f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Then, we create a PySpark ML Pipeline to preprocess the data and apply transformations systematically\n",
    "pipeline = Pipeline(stages=indexers + [feature_vector])\n",
    "data = pipeline.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0e511c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.select([\"features\",\"Offence_Level_index\"])\n",
    "input_data = input_data.withColumnRenamed(\"Offence_Level_index\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05b9b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, we divide our data into 0.7 train, 0.3 test\n",
    "train_data, test_data = input_data.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbe342",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8636f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e321c128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 687:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9351692365772801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lg_model = LogisticRegression(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    maxIter=100,  \n",
    "    elasticNetParam=0,\n",
    "    regParam=0.1,       \n",
    "    family='multinomial'\n",
    ")\n",
    "\n",
    "lg_model = lg_model.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lg_pred = lg_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_evaluator.evaluate(lg_pred)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195735f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43d8c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_predictions = data.join(lg_pred.select(\"prediction\", \"features\"), on=\"features\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0cf64f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|Victim_Sex|Prediction_Rate_Pre|\n",
      "+----------+-------------------+\n",
      "|         F| 0.2226985383285381|\n",
      "|         E| 0.3330672634234645|\n",
      "|         M|0.33337901764003713|\n",
      "|         L| 0.7190653212958046|\n",
      "|         D|0.08519874216289901|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1109:=====================================>                  (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|Victim_Agegroup|Prediction_Rate_Pre|\n",
      "+---------------+-------------------+\n",
      "|         Senior|0.42009685230024213|\n",
      "|       Teenager| 0.3832132259431963|\n",
      "|     Middle Age| 0.1691738686604233|\n",
      "|    Young Adult|0.32704257767548905|\n",
      "|        Mid Old| 0.2632832463633883|\n",
      "+---------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pre-mitigation: Calculate Demographic Parity for Victim_Sex\n",
    "dp_sex_pre = data_with_predictions.groupBy(\"Victim_Sex\") \\\n",
    "    .agg(\n",
    "        (count(when(col(\"prediction\") == 1.0, True)) / count(\"*\")).alias(\"Prediction_Rate_Pre\")\n",
    "    )\n",
    "\n",
    "# Pre-mitigation: Calculate Demographic Parity for Victim_Agegroup\n",
    "dp_age_pre = data_with_predictions.groupBy(\"Victim_Agegroup\") \\\n",
    "    .agg(\n",
    "        (count(when(col(\"prediction\") == 1.0, True)) / count(\"*\")).alias(\"Prediction_Rate_Pre\")\n",
    "    )\n",
    "\n",
    "# Show pre-mitigation metrics\n",
    "dp_sex_pre.show()\n",
    "dp_age_pre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "420b4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to Pandas DataFrames\n",
    "dp_sex_pre_pd = dp_sex_pre.toPandas()\n",
    "dp_age_pre_pd = dp_age_pre.toPandas()\n",
    "\n",
    "# Plot Demographic Parity for Victim_Sex (Pre-Mitigation)\n",
    "dp_sex_pre_pd.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Sex',\n",
    "    y='Prediction_Rate_Pre',\n",
    "    title='Demographic Parity Pre-Mitigation by Victim Sex'\n",
    ")\n",
    "plt.xlabel('Victim Sex')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Demographic Parity for Victim_Agegroup (Pre-Mitigation)\n",
    "dp_age_pre_pd.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Agegroup',\n",
    "    y='Prediction_Rate_Pre',\n",
    "    title='Demographic Parity Pre-Mitigation by Victim Agegroup'\n",
    ")\n",
    "plt.xlabel('Victim Agegroup')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c91e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.select([\"features\", \"Offence_Level_index\", \"Victim_Sex\", \"Victim_Agegroup\"])\n",
    "input_data = input_data.withColumnRenamed(\"Offence_Level_index\", \"label\")\n",
    "\n",
    "# Split into training and test sets\n",
    "train_data, test_data = input_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "589f177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lg_model = LogisticRegression(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",  # Use weights if applicable\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0\n",
    ")\n",
    "\n",
    "lg_model = lg_model.fit(weighted_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5add442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1206:========================>                               (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.8515617376493653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = lg_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_evaluator.evaluate(lg_pred)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {acc}\")\n",
    "\n",
    "# Include protected attributes for fairness analysis\n",
    "predictions_with_attributes = predictions.select(\"prediction\", \"label\", \"Victim_Sex\", \"Victim_Agegroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12f5bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_with_attributes = predictions.select(\"prediction\", \"label\", \"Victim_Sex\", \"Victim_Agegroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4fec4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Victim_Sex|Prediction_Rate_Post|\n",
      "+----------+--------------------+\n",
      "|         F|  0.3115037449587094|\n",
      "|         E|   0.383236739322162|\n",
      "|         M|  0.3932356065434459|\n",
      "|         L|  0.6983273596176822|\n",
      "|         D|  0.2894621237414117|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1216:>                                                       (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|Victim_Agegroup|Prediction_Rate_Post|\n",
      "+---------------+--------------------+\n",
      "|         Senior| 0.42495374653098983|\n",
      "|       Teenager| 0.42037803196235907|\n",
      "|     Middle Age|  0.3373557114832558|\n",
      "|    Young Adult|  0.3523597726711144|\n",
      "|        Mid Old| 0.33088758861954737|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1216:=================================================>      (8 + 1) / 9]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Demographic Parity for Victim_Sex (Post-Mitigation)\n",
    "dp_sex_post = predictions_with_attributes.groupBy(\"Victim_Sex\") \\\n",
    "    .agg(\n",
    "        (count(when(col(\"prediction\") == 1.0, True)) / count(\"*\")).alias(\"Prediction_Rate_Post\")\n",
    "    )\n",
    "\n",
    "# Demographic Parity for Victim_Agegroup (Post-Mitigation)\n",
    "dp_age_post = predictions_with_attributes.groupBy(\"Victim_Agegroup\") \\\n",
    "    .agg(\n",
    "        (count(when(col(\"prediction\") == 1.0, True)) / count(\"*\")).alias(\"Prediction_Rate_Post\")\n",
    "    )\n",
    "\n",
    "# Show post-mitigation metrics\n",
    "dp_sex_post.show()\n",
    "dp_age_post.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1f37b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to Pandas DataFrames\n",
    "dp_sex_post_pd = dp_sex_post.toPandas()\n",
    "dp_age_post_pd = dp_age_post.toPandas()\n",
    "\n",
    "# Plot Demographic Parity for Victim_Sex (Post-Mitigation)\n",
    "dp_sex_post_pd.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Sex',\n",
    "    y='Prediction_Rate_Post',\n",
    "    title='Demographic Parity Post-Mitigation by Victim Sex'\n",
    ")\n",
    "plt.xlabel('Victim Sex')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Demographic Parity for Victim_Agegroup (Post-Mitigation)\n",
    "dp_age_post_pd.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Agegroup',\n",
    "    y='Prediction_Rate_Post',\n",
    "    title='Demographic Parity Post-Mitigation by Victim Agegroup'\n",
    ")\n",
    "plt.xlabel('Victim Agegroup')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da00f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Victim_Sex  Prediction_Rate_Pre  Prediction_Rate_Post\n",
      "0          F             0.222699              0.311504\n",
      "1          E             0.333067              0.383237\n",
      "2          M             0.333379              0.393236\n",
      "3          L             0.719065              0.698327\n",
      "4          D             0.085199              0.289462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure both DataFrames have 'Victim_Sex' as a column\n",
    "# Convert to Pandas if not already\n",
    "dp_sex_pre_pd = dp_sex_pre.toPandas()\n",
    "dp_sex_post_pd = dp_sex_post.toPandas()\n",
    "\n",
    "# Merge on 'Victim_Sex'\n",
    "dp_sex_comparison = pd.merge(dp_sex_pre_pd, dp_sex_post_pd, on='Victim_Sex')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(dp_sex_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd82c089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Victim_Agegroup  Prediction_Rate_Pre  Prediction_Rate_Post\n",
      "0          Senior             0.420097              0.424954\n",
      "1        Teenager             0.383213              0.420378\n",
      "2      Middle Age             0.169174              0.337356\n",
      "3     Young Adult             0.327043              0.352360\n",
      "4         Mid Old             0.263283              0.330888\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas if not already\n",
    "dp_age_pre_pd = dp_age_pre.toPandas()\n",
    "dp_age_post_pd = dp_age_post.toPandas()\n",
    "\n",
    "# Merge on 'Victim_Agegroup'\n",
    "dp_age_comparison = pd.merge(dp_age_pre_pd, dp_age_post_pd, on='Victim_Agegroup')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(dp_age_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c19f2371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Demographic Parity Comparison for Victim_Sex\n",
    "dp_sex_comparison.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Sex',\n",
    "    y=['Prediction_Rate_Pre', 'Prediction_Rate_Post'],\n",
    "    title='Demographic Parity Comparison for Victim Sex'\n",
    ")\n",
    "plt.xlabel('Victim Sex')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f07b21c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:36:06 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2034859 ms exceeds timeout 120000 ms\n",
      "24/12/09 17:36:06 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "24/12/09 17:36:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:36:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:50:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:50:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:50:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:50:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:50:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:50:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:51:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:51:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:51:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:51:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:51:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:51:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:51:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:52:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:52:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:52:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:52:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:52:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:52:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:52:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:53:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:53:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:54:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:54:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:54:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:54:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:54:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:54:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:54:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:55:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:55:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:55:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:55:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:55:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:55:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:55:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:56:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:56:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:56:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:56:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:56:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:56:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:56:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:57:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:57:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:57:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:57:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:57:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:57:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:57:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:58:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:58:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:58:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 17:58:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 17:58:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 17:58:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 18:18:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:18:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:18:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 18:18:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 18:18:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:18:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:18:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:19:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:19:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:19:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:19:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:19:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:19:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:19:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/12/09 18:19:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:19:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 18:19:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.31.31:55929\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/12/09 18:19:40 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "# Plot Demographic Parity Comparison for Victim_Agegroup\n",
    "dp_age_comparison.plot(\n",
    "    kind='bar',\n",
    "    x='Victim_Agegroup',\n",
    "    y=['Prediction_Rate_Pre', 'Prediction_Rate_Post'],\n",
    "    title='Demographic Parity Comparison for Victim Agegroup'\n",
    ")\n",
    "plt.xlabel('Victim Agegroup')\n",
    "plt.ylabel('Prediction Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
